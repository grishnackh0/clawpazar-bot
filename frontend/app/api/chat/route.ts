// ClawPazar â€“ Server-side LLM proxy for Agent Chat
// Uses Zhipu AI GLM-5.0-72B via OpenAI-compatible endpoint
// Streams responses back to the client via SSE

import { NextRequest, NextResponse } from 'next/server';

/* â”€â”€ System Prompt â”€â”€ */

const SYSTEM_PROMPT = `Sen ClawPazar'sÄ±n â€” TÃ¼rkiye'nin en hÄ±zlÄ± ve en gÃ¼venilir ikinci el alÄ±ÅŸveriÅŸ platformunun AI satÄ±ÅŸ koÃ§u.

SEN CHATBOT DEÄžÄ°LSÄ°N. Sen gerÃ§ek bir pazaryeri asistanÄ±sÄ±n. KullanÄ±cÄ±ya satÄ±ÅŸ heyecanÄ± ver, alÄ±ÅŸveriÅŸ enerjisi kat.

## KÄ°ÅžÄ°LÄ°ÄžÄ°N
- Enerjik, samimi, gÃ¼ven veren. ArkadaÅŸÄ±na tavsiye verir gibi konuÅŸ.
- "Bu fÄ±rsat kaÃ§maz!", "Harika seÃ§im!", "Bunu hÄ±zlÄ± kapan kazanÄ±r!" gibi ifadeler kullan.
- Her mesajÄ±n sonunda kullanÄ±cÄ±yÄ± bir sonraki adÄ±ma yÃ¶nlendir.
- KÄ±sa ve vurucu cÃ¼mleler. Max 3-4 satÄ±r.

## SATIÅž AKIÅžI (form deÄŸil, doÄŸal sohbet)
KullanÄ±cÄ± Ã¼rÃ¼n satmak istediÄŸinde:
1. Heyecanla karÅŸÄ±la: "Harika! [Ã¼rÃ¼n] ÅŸu an Ã§ok aranan bir Ã¼rÃ¼n ðŸ”¥"
2. Eksik bilgileri DOÄžAL ÅŸekilde sor â€” form gibi deÄŸil, sohbet gibi
3. Fiyat Ã¶nerisi ver: "Piyasada bu model 25-30K arasÄ± gidiyor, senin fiyatÄ±n Ã§ok iyi!"
4. Ä°lan taslaÄŸÄ±nÄ± heyecanla sun ve "Bu ilan saatler iÃ§inde 500+ kiÅŸiye ulaÅŸacak! YayÄ±nlayalÄ±m mÄ±? ðŸš€" de

## ALICI AKIÅžI
- "Hemen bakalÄ±m! Harika fÄ±rsatlar var" ile baÅŸla
- BÃ¼tÃ§e ve tercih sor, uygun Ã¶neriler sun

## KARGO & GÃœVENLÄ°K
- Ä°lan sonrasÄ± doÄŸal hatÄ±rlat: "Kargo ve Ã¶deme konusunda endiÅŸelenme. GÃ¼venli kargo sistemiyle Ã¼rÃ¼nÃ¼n sigortalÄ± gÃ¶nderiliyor ðŸ“¦"
- Kargo seÃ§enekleri: SÃ¼rat 40â‚º, Aras 42â‚º, YurtiÃ§i 45â‚º
- Escrow: "ParanÄ±z gÃ¼vende tutulur. AlÄ±cÄ± onaylayana kadar kimse dokunmaz âœ…"

## YASAKLAR
- AynÄ± soruyu ASLA tekrarlama
- Form dolduruyor hissi verme
- Robotik konuÅŸma
- TC, IBAN, banka bilgisi ASLA isteme
- ÅžÃ¼pheli fiyat/Ã¶deme talebi varsa uyar`;

/* â”€â”€ Types â”€â”€ */

interface ChatRequestBody {
  messages: { role: string; content: string }[];
}

/* â”€â”€ Route Handler â”€â”€ */

export async function POST(req: NextRequest) {
  const apiKey = process.env.ZHIPU_API_KEY;
  const apiBase = process.env.ZHIPU_API_BASE || 'https://open.bigmodel.cn/api/paas/v4';
  const model = process.env.ZHIPU_MODEL || 'glm-5.0-72b';

  if (!apiKey) {
    return NextResponse.json(
      { error: 'LLM API key yapÄ±landÄ±rÄ±lmamÄ±ÅŸ. .env dosyasÄ±na ZHIPU_API_KEY ekleyin.' },
      { status: 500 }
    );
  }

  let body: ChatRequestBody;
  try {
    body = await req.json();
  } catch {
    return NextResponse.json({ error: 'GeÃ§ersiz istek gÃ¶vdesi' }, { status: 400 });
  }

  if (!body.messages || !Array.isArray(body.messages) || body.messages.length === 0) {
    return NextResponse.json({ error: 'messages dizisi gerekli' }, { status: 400 });
  }

  // Build messages array with system prompt + conversation history
  const llmMessages = [
    { role: 'system', content: SYSTEM_PROMPT },
    ...body.messages.map((m) => ({
      role: m.role === 'agent' ? 'assistant' : m.role,
      content: m.content,
    })),
  ];

  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 60_000); // 60s timeout

    const response = await fetch(`${apiBase}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages: llmMessages,
        stream: true,
        temperature: 0.7,
        max_tokens: 1024,
        top_p: 0.9,
      }),
      signal: controller.signal,
    });

    clearTimeout(timeout);

    if (!response.ok) {
      const errText = await response.text().catch(() => 'Unknown error');
      console.error(`[LLM] API error ${response.status}: ${errText}`);
      return NextResponse.json(
        { error: `LLM API hatasÄ± (${response.status}): ${errText}` },
        { status: 502 }
      );
    }

    if (!response.body) {
      return NextResponse.json({ error: 'LLM yanÄ±t stream yok' }, { status: 502 });
    }

    // Stream the SSE response back to client
    const encoder = new TextEncoder();
    const decoder = new TextDecoder();

    const readable = new ReadableStream({
      async start(streamController) {
        const reader = response.body!.getReader();
        let buffer = '';

        try {
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
              const trimmed = line.trim();
              if (!trimmed || !trimmed.startsWith('data: ')) continue;

              const data = trimmed.slice(6);
              if (data === '[DONE]') {
                streamController.enqueue(encoder.encode('data: [DONE]\n\n'));
                continue;
              }

              try {
                const json = JSON.parse(data);
                const content = json.choices?.[0]?.delta?.content;
                if (content) {
                  streamController.enqueue(
                    encoder.encode(`data: ${JSON.stringify({ content })}\n\n`)
                  );
                }
              } catch {
                // Skip malformed JSON chunks
              }
            }
          }
        } catch (err) {
          console.error('[LLM] Stream read error:', err);
        } finally {
          streamController.close();
        }
      },
    });

    return new Response(readable, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        Connection: 'keep-alive',
      },
    });
  } catch (err: unknown) {
    if (err instanceof Error && err.name === 'AbortError') {
      return NextResponse.json({ error: 'LLM isteÄŸi zaman aÅŸÄ±mÄ±na uÄŸradÄ± (60s)' }, { status: 504 });
    }
    console.error('[LLM] Unexpected error:', err);
    return NextResponse.json({ error: 'Beklenmeyen LLM hatasÄ±' }, { status: 500 });
  }
}
